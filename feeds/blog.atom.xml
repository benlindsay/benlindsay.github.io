<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ben Lindsay - Blog</title><link href="http://benjlindsay.com/" rel="alternate"></link><link href="http://benjlindsay.com/feeds/blog.atom.xml" rel="self"></link><id>http://benjlindsay.com/</id><updated>2018-04-30T11:19:27-04:00</updated><entry><title>Running Jupyter Lab Remotely</title><link href="http://benjlindsay.com/blog/running-jupyter-lab-remotely/" rel="alternate"></link><published>2018-04-30T11:19:27-04:00</published><updated>2018-04-30T11:19:27-04:00</updated><author><name>Ben Lindsay</name></author><id>tag:benjlindsay.com,2018-04-30:/blog/running-jupyter-lab-remotely/</id><summary type="html">&lt;p&gt;I'm a huge fan of &lt;a href="http://jupyter.org/"&gt;Jupyter Notebooks&lt;/a&gt;, and I was very excited when I found out about Jupyter Lab, which provides a much more comprehensive user experience around Jupyter Notebooks. &lt;a href="https://towardsdatascience.com/jupyter-notebooks-are-breathtakingly-featureless-use-jupyter-lab-be858a67b59d"&gt;Other&lt;/a&gt; &lt;a href="https://blog.jupyter.org/jupyterlab-is-ready-for-users-5a6f039b8906"&gt;posts&lt;/a&gt; have covered in more detail why we should switch to using Jupyter Lab instead, so I won't talk …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I'm a huge fan of &lt;a href="http://jupyter.org/"&gt;Jupyter Notebooks&lt;/a&gt;, and I was very excited when I found out about Jupyter Lab, which provides a much more comprehensive user experience around Jupyter Notebooks. &lt;a href="https://towardsdatascience.com/jupyter-notebooks-are-breathtakingly-featureless-use-jupyter-lab-be858a67b59d"&gt;Other&lt;/a&gt; &lt;a href="https://blog.jupyter.org/jupyterlab-is-ready-for-users-5a6f039b8906"&gt;posts&lt;/a&gt; have covered in more detail why we should switch to using Jupyter Lab instead, so I won't talk about that here.&lt;/p&gt;
&lt;p&gt;Instead, I just want to share how to run Jupyter Lab efficiently on a remote machine. I have a research cluster where I do most of my analyses for my PhD work, and running Jupyter Lab directly on the cluster means I don't have to copy files betw een the cluster and my desktop.&lt;/p&gt;
&lt;h2&gt;The basic commands&lt;/h2&gt;
&lt;p&gt;To run Jupyter Lab on a remote machine, you need to open 2 terminal windows. In the first window:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ssh username@hostname
$ jupyter lab --no-browser --port&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;8888&lt;/span&gt;
...
&lt;span class="o"&gt;[&lt;/span&gt;I &lt;span class="m"&gt;10&lt;/span&gt;:17:14.160 LabApp&lt;span class="o"&gt;]&lt;/span&gt; Use Control-C to stop this server and shut down all kernels &lt;span class="o"&gt;(&lt;/span&gt;twice to skip confirmation&lt;span class="o"&gt;)&lt;/span&gt;.
&lt;span class="o"&gt;[&lt;/span&gt;C &lt;span class="m"&gt;10&lt;/span&gt;:17:14.160 LabApp&lt;span class="o"&gt;]&lt;/span&gt;

    Copy/paste this URL into your browser when you connect &lt;span class="k"&gt;for&lt;/span&gt; the first time,
    to login with a token:
        http://localhost:8888/?token&lt;span class="o"&gt;=&lt;/span&gt;b6ff64ea67275581a2ec91790e1ed591c945f30ee0f7a214
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then in the second window:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ssh -Y -N -L localhost:8888:localhost:8888 username@hostname
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then in your web browser of choice, copy&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;http://localhost:8888/?token=b6ff64ea67275581a2ec91790e1ed591c945f30ee0f7a214
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;in the url bar. You could just copy &lt;code&gt;localhost:8888&lt;/code&gt;, but then it might ask for your token, which you'd have to copy and paste from your remote machine.&lt;/p&gt;
&lt;p&gt;All that is kind of a lot just to open up Jupyter Lab. So I found ways to significantly simplify the process from both the remote and local side.&lt;/p&gt;
&lt;h2&gt;Simplfying the remote side&lt;/h2&gt;
&lt;p&gt;To make things easier on the remote machine side of things, &lt;code&gt;tmux&lt;/code&gt; (or &lt;code&gt;screen&lt;/code&gt;) and aliases really come in handy. I like to have as Jupyter Lab session running constantly in my remote machine whether I'm logged in or not. Then I can just ssh tunnel in to the existing session whenever I want! To do this, I simply do the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ssh username@hostname
$ tmux
&lt;span class="o"&gt;[&lt;/span&gt; opens persistent shell session &lt;span class="o"&gt;]&lt;/span&gt;
$ jlremote
...
&lt;span class="o"&gt;[&lt;/span&gt;I &lt;span class="m"&gt;10&lt;/span&gt;:17:14.160 LabApp&lt;span class="o"&gt;]&lt;/span&gt; Use Control-C to stop this server and shut down all kernels &lt;span class="o"&gt;(&lt;/span&gt;twice to skip confirmation&lt;span class="o"&gt;)&lt;/span&gt;.
&lt;span class="o"&gt;[&lt;/span&gt;C &lt;span class="m"&gt;10&lt;/span&gt;:17:14.160 LabApp&lt;span class="o"&gt;]&lt;/span&gt;

    Copy/paste this URL into your browser when you connect &lt;span class="k"&gt;for&lt;/span&gt; the first time,
    to login with a token:
        http://localhost:8888/?token&lt;span class="o"&gt;=&lt;/span&gt;b6ff64ea67275581a2ec91790e1ed591c945f30ee0f7a214
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I have &lt;code&gt;jlremote&lt;/code&gt; defined as an alias in my remote &lt;code&gt;~/.bashrc&lt;/code&gt; file like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;alias jlremote=&amp;#39;jupyter lab --no-browser --port=8888&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So once I have that Jupyater Lab session running, I can detach from the tmux session with &lt;code&gt;CTRL-b, d&lt;/code&gt; (or &lt;code&gt;CTRL-a, CTRL-d&lt;/code&gt; if you used the &lt;code&gt;screen&lt;/code&gt; command), and let that process run indefinitely (days, weeks, months...).&lt;/p&gt;
&lt;p&gt;Now let's deal with the local stuff.&lt;/p&gt;
&lt;h2&gt;Simplfying the local side&lt;/h2&gt;
&lt;p&gt;On the local side, I wanted to be able to just run a single command like &lt;code&gt;jllocal&lt;/code&gt; to open Jupyter Lab, so I wrote a bash function that goes in my local &lt;code&gt;~/.bashrc&lt;/code&gt; file. If you use this make sure to edit all the all-caps stuff, like &lt;code&gt;USERNAME&lt;/code&gt;, &lt;code&gt;HOSTNAME&lt;/code&gt;, and &lt;code&gt;REMOTE/PATH/TO/jupyter&lt;/code&gt;. The Jupyter path should be something like &lt;code&gt;~/anaconda3/bin/jupyter&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;jllocal&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;cmd&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ssh -Y -fN -L localhost:8888:localhost:8888 USERNAME@HOSTNAME&amp;quot;&lt;/span&gt;
  &lt;span class="nx"&gt;running_cmds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;$&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ps&lt;/span&gt; &lt;span class="nx"&gt;aux&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="nx"&gt;grep&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;v&lt;/span&gt; &lt;span class="nx"&gt;grep&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="nx"&gt;grep&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;$cmd&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;$1&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;kill&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;]];&lt;/span&gt; &lt;span class="nx"&gt;then&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="o"&gt;!&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;z&lt;/span&gt; &lt;span class="nx"&gt;$running_cmds&lt;/span&gt; &lt;span class="p"&gt;];&lt;/span&gt; &lt;span class="nx"&gt;then&lt;/span&gt;
      &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;pid&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;$&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;echo&lt;/span&gt; &lt;span class="nx"&gt;$running_cmds&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="nx"&gt;awk&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;{print $2}&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
        &lt;span class="nx"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;killing pid $pid&amp;quot;&lt;/span&gt;
        &lt;span class="nx"&gt;kill&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt; &lt;span class="nx"&gt;$pid&lt;/span&gt;
      &lt;span class="nx"&gt;done&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;
      &lt;span class="nx"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;No jllocal commands to kill.&amp;quot;&lt;/span&gt;
    &lt;span class="nx"&gt;fi&lt;/span&gt;
  &lt;span class="k"&gt;else&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="o"&gt;!&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;z&lt;/span&gt; &lt;span class="nx"&gt;$n_running_cmds&lt;/span&gt; &lt;span class="p"&gt;];&lt;/span&gt; &lt;span class="nx"&gt;then&lt;/span&gt;
      &lt;span class="nx"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;jllocal command is still running. Kill with &amp;#39;jllocal kill&amp;#39; next time.&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;
      &lt;span class="nx"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Running command &amp;#39;$cmd&amp;#39;&amp;quot;&lt;/span&gt;
      &lt;span class="nb"&gt;eval&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;$cmd&amp;quot;&lt;/span&gt;
    &lt;span class="nx"&gt;fi&lt;/span&gt;
    &lt;span class="nx"&gt;url&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;$&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ssh&lt;/span&gt; &lt;span class="nx"&gt;USERNAME&lt;/span&gt;&lt;span class="kd"&gt;@HOSTNAME&lt;/span&gt; &lt;span class="o"&gt;\&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;/REMOTE/PATH/TO/jupyter notebook list&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;\&lt;/span&gt;
            &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="nx"&gt;grep&lt;/span&gt; &lt;span class="nx"&gt;http&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="nx"&gt;awk&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;{print $1}&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nx"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;URL that will open in your browser:&amp;quot;&lt;/span&gt;
    &lt;span class="nx"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;$url&amp;quot;&lt;/span&gt;
    &lt;span class="nx"&gt;open&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;$url&amp;quot;&lt;/span&gt;
  &lt;span class="nx"&gt;fi&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This function does a few things when you type &lt;code&gt;jllocal&lt;/code&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Runs ssh tunneling command if it's not already running&lt;/li&gt;
&lt;li&gt;Grabs the Jupyter token from the remote machine&lt;/li&gt;
&lt;li&gt;Opens a tab in your browser with the right url and token for you&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When you're done with Jupyter Lab, you just type &lt;code&gt;jllocal kill&lt;/code&gt; and it will shut down the ssh connection.&lt;/p&gt;
&lt;h1&gt;Putting it all together&lt;/h1&gt;
&lt;p&gt;So with a simple alias in place in your remote &lt;code&gt;~/.bashrc&lt;/code&gt;, a persistent remote tmux/screen session running Jupyter Lab, and a function defined in your local &lt;code&gt;~/.bashrc&lt;/code&gt;, all you need to do to open Jupyter Lab in your browser is a simple &lt;code&gt;jllocal&lt;/code&gt; on your local machine, and then &lt;code&gt;jllocal kill&lt;/code&gt; when you're done. It takes some initial set up work, but the simplicity in the end is worth it.&lt;/p&gt;</content><category term="bash"></category><category term="jupyter"></category><category term="tmux"></category><category term="ssh"></category><category term="productivity"></category></entry><entry><title>Analyzing Larger-than-Memory Data on your Laptop</title><link href="http://benjlindsay.com/blog/analyzing-larger-than-memory-data-on-your-laptop/" rel="alternate"></link><published>2017-03-10T19:10:00-05:00</published><updated>2017-03-10T19:10:00-05:00</updated><author><name>Ben Lindsay</name></author><id>tag:benjlindsay.com,2017-03-10:/blog/analyzing-larger-than-memory-data-on-your-laptop/</id><summary type="html">&lt;p&gt;If you want to run some analysis on a dataset that's just a little too big to load into memory on your laptop, but you don't want to leave the comfort of using &lt;a class="reference external" href="http://pandas.pydata.org/"&gt;Pandas&lt;/a&gt; dataframes in a &lt;a class="reference external" href="http://jupyter.org/"&gt;Jupyter&lt;/a&gt; notebook, then &lt;a class="reference external" href="http://dask.pydata.org/"&gt;Dask&lt;/a&gt; may be just your thing. Dask is an amazing …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you want to run some analysis on a dataset that's just a little too big to load into memory on your laptop, but you don't want to leave the comfort of using &lt;a class="reference external" href="http://pandas.pydata.org/"&gt;Pandas&lt;/a&gt; dataframes in a &lt;a class="reference external" href="http://jupyter.org/"&gt;Jupyter&lt;/a&gt; notebook, then &lt;a class="reference external" href="http://dask.pydata.org/"&gt;Dask&lt;/a&gt; may be just your thing. Dask is an amazing Python library that lets you do all your Pandas-style dataframe manipulations with just a few simple tweaks so you don't have to worry about Jupyter freezing up.&lt;/p&gt;
&lt;p&gt;I'll demonstrate the benefits of Dask and some of its syntax by running a calculation on business reviews provided for the &lt;a class="reference external" href="https://www.yelp.com/dataset_challenge"&gt;Yelp Dataset Challenge&lt;/a&gt;, which contains 3.6 million business reviews. The reviews were provided in a file where each line is a JSON object with keys that include &lt;code&gt;&amp;quot;business_id&amp;quot;&lt;/code&gt;,  &lt;code&gt;&amp;quot;user_id&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;review_id&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;stars&amp;quot;&lt;/code&gt;, and others. I extracted about 90% of all the JSON objects associated with businesses in Champaign, Illinois to one file as a small dataset that can be loaded into Pandas, and about 90% of all the JSON objects associated with any US/Canada business into another file as a larger dataset that does not fit into a Pandas dataframe on my laptop. You can view the notebook with all the code below &lt;a class="reference external" href="https://github.com/benlindsay/yelp-dataset-challenge/blob/master/ben-notebooks/pandas_dask_comparison.ipynb"&gt;here on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="baseline-prediction-method"&gt;
&lt;h2&gt;Baseline Prediction Method&lt;/h2&gt;
&lt;p&gt;The baseline prediction method I'll show below is one of 4 methods discussed in &lt;a class="reference external" href="http://files.grouplens.org/papers/FnT%20CF%20Recsys%20Survey.pdf"&gt;this excellent survey of collaborative filtering recommender systems&lt;/a&gt; by Michael Ekstrand, John Riedl, and Joseph Konstan. The methods are:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Predict by user's average rating&lt;/li&gt;
&lt;li&gt;Predict by item's average rating (&amp;quot;items&amp;quot; are businesses in this case)&lt;/li&gt;
&lt;li&gt;Predict by user's and item's average ratings&lt;/li&gt;
&lt;li&gt;Predict by user's and item's average ratings with damping factors&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The 4th method ended up giving the best predictions on both the Champaign data and US/Canada training set. The damping factors reduce the weight placed on users or items with few reviews, making the prediction more robust. The necessary equations are 2.1, 2.4, and 2.5 in the survey linked above.&lt;/p&gt;
&lt;p&gt;Equation 2.1 (&lt;span class="math"&gt;\(b_{u,i} = \mu + b_u + b_i\)&lt;/span&gt;) essentially says that if we want the baseline prediction for user &lt;span class="math"&gt;\(u\)&lt;/span&gt;'s rating of item &lt;span class="math"&gt;\(i\)&lt;/span&gt;, we can sum up the total average &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;, the offset from the &lt;span class="math"&gt;\(\mu\)&lt;/span&gt;  corresponding to user &lt;span class="math"&gt;\(u\)&lt;/span&gt; (&lt;span class="math"&gt;\(b_u\)&lt;/span&gt;), and the offset from &lt;span class="math"&gt;\(\mu + b_u\)&lt;/span&gt; corresponding to item &lt;span class="math"&gt;\(i\)&lt;/span&gt; (&lt;span class="math"&gt;\(b_i\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The equations for &lt;span class="math"&gt;\(b_u\)&lt;/span&gt; and &lt;span class="math"&gt;\(b_i\)&lt;/span&gt; are&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
b_u = \frac{1}{|I_u| + \beta_u}\sum_{i \in I_u} (r_{u,i} - \mu)
\end{equation*}
&lt;/div&gt;
&lt;div class="math"&gt;
\begin{equation*}
b_i = \frac{1}{|U_i| + \beta_i}\sum_{u \in U_i} (r_{u,i} - b_u - \mu)
\end{equation*}
&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(r_{u_i}\)&lt;/span&gt; is the actual rating of item (business) &lt;span class="math"&gt;\(i\)&lt;/span&gt; given by user &lt;span class="math"&gt;\(u\)&lt;/span&gt;, &lt;span class="math"&gt;\(I_u\)&lt;/span&gt; is the set of items rated by user &lt;span class="math"&gt;\(u\)&lt;/span&gt;, and &lt;span class="math"&gt;\(U_i\)&lt;/span&gt; is the set of users who rated business &lt;span class="math"&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="loading-data"&gt;
&lt;h2&gt;Loading Data&lt;/h2&gt;
&lt;p&gt;For all the following code blocks, assume we have the following imports:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;dask.bag&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;db&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;First, let's compare the data loading process for the small and large datasets. In both cases, the data are in the form of a single file with one line of JSON data for each review. Loading the Champaign data using Pandas looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;df_rev&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_json&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;../preprocessed-data/all-champaign-reviews.json&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;orient&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;records&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lines&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df_rev_champaign&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df_rev_champaign&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;review_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;business_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;user_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;stars&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For the larger US/Canada training set, loading the data using Dask looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dict_bag&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;db&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_text&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;../preprocessed-data/reviews_train.json&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;blocksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;5e6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df_rev&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dict_bag&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_dataframe&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;review_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;business_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;user_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;stars&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;df_rev&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df_rev&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;repartition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;npartitions&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When loading in larger-than-memory data, Dask splits the data into partitions no larger than &lt;code&gt;blocksize&lt;/code&gt;. You want to ensure you have enough partitions to ensure your computer doesn't freeze, but too many will slow down the computation. For that reason, after I make a dataframe from a small subset of the features I read in, I repartition the data to reduce the number of partitions to 10. After the data are loaded in, you can treat your Dask datafame just like a Pandas dataframe (for the most part).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="computing-prediction-error"&gt;
&lt;h2&gt;Computing Prediction Error&lt;/h2&gt;
&lt;p&gt;For these baseline tests, I use the root mean squared error (RMSE) to measure the baseline accuracy. When dealing with Pandas dataframes, I can use a function like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rmse_pandas&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;diff_sq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;diff_sq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In Dask, I can do the same thing with just an extra &lt;code&gt;.compute()&lt;/code&gt; added, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rmse_dask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;diff_sq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;diff_sq&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compute&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is necessary because Dask uses &amp;quot;lazy evaluation&amp;quot; by default, and only computes results when you tell it to.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="splitting-dataframe-into-train-and-test-sets"&gt;
&lt;h2&gt;Splitting Dataframe into Train and Test Sets&lt;/h2&gt;
&lt;p&gt;Splitting the Pandas dataframe:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="n"&gt;df_train_champaign&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df_test_champaign&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_rev_champaign&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Splitting the Dask dataframe:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;df_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df_rev&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random_split&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Unfortunately we can't use Scikit-learn on Dask dataframes, but a lot of the essential capabilities of Scikit-learn are implemented in Dask, or Dask compatible libraries.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="computing-baselines"&gt;
&lt;h2&gt;Computing Baselines&lt;/h2&gt;
&lt;p&gt;Now here's the exciting part: the actual baseline computation uses the exact same code no matter whether it's a Dask or Pandas dataframe. Here's the function that computes the baseline predictions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_baseline_rmse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta_u&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta_i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rmse_func&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    df_train and df_test are either Pandas or Dask dataframes&lt;/span&gt;
&lt;span class="sd"&gt;    that must contain the columns &amp;#39;user_id&amp;#39;, &amp;#39;business_id&amp;#39;, and &amp;#39;stars&amp;#39;.&lt;/span&gt;
&lt;span class="sd"&gt;    beta_u and beta_i are user and business damping factors, respectively.&lt;/span&gt;
&lt;span class="sd"&gt;    rmse_func is a function that computes the RMSE of the prediction&lt;/span&gt;
&lt;span class="sd"&gt;    and takes Pandas or Dask Series objects, depending on whether&lt;/span&gt;
&lt;span class="sd"&gt;    df_train and df_test are Pandas or Dask Dataframes.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# Get mean rating of all training ratings&lt;/span&gt;
    &lt;span class="n"&gt;train_mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;stars&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="c1"&gt;# Get dataframe of b_u part of baseline for each user id&lt;/span&gt;
    &lt;span class="n"&gt;user_group&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df_train&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;user_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;stars&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;user_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df_train_user&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;user_group&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;stars&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;df_train_user&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;b_u&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_train_user&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;train_mean&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;df_train_user&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;df_train_user&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;b_u&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_train_user&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;beta_u&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# Create column of b_u values corresponding to the user who made the review&lt;/span&gt;
    &lt;span class="n"&gt;df_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_train_user&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;b_u&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;user_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# Add column representing the expression inside the summation part of the b_i equation&lt;/span&gt;
    &lt;span class="n"&gt;df_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;b_i_sum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;stars&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;df_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;b_u&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;train_mean&lt;/span&gt;
    &lt;span class="c1"&gt;# Average over each business to get the actual b_i values for each business&lt;/span&gt;
    &lt;span class="n"&gt;bus_group&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df_train&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;business_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;b_i_sum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;business_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df_train_bus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bus_group&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;b_i_sum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rename&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;b_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="n"&gt;df_train_bus&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;b_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;df_train_bus&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;beta_i&lt;/span&gt;
    &lt;span class="c1"&gt;# Join b_u and b_i columns to test dataframe&lt;/span&gt;
    &lt;span class="n"&gt;df_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_train_user&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;b_u&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;user_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fillna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_train_user&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;b_u&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;df_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_train_bus&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;b_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;business_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fillna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_train_bus&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;b_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="c1"&gt;# Predict and Compute error&lt;/span&gt;
    &lt;span class="n"&gt;df_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;b_u&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;df_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;b_i&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;train_mean&lt;/span&gt;
    &lt;span class="n"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rmse_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;stars&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;df_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pred&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Error = {}&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I call that function using either&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;compute_baseline_rmse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_train_champaign&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df_test_champaign&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rmse_pandas&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;for the Champaign Pandas dataframes or&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;compute_baseline_rmse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rmse_dask&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;for the US/Canada Dask dataframes. Note that even relatively simple calculations like these can still take a long time if you're just running on your laptop, especially if you more partitions than necessary.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;If you want to do dataframe manipulations or standard machine learning on a dataset that's just a little bigger than the memory you have available, I highly recommend Dask. For more complex computations or bigger datasets, you might want to stick with something fancier like Spark clusters in the cloud.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="acknowledgments"&gt;
&lt;h2&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;Thanks to &lt;a class="reference external" href="http://arielrodriguezromero.com/"&gt;Ariel Rodriquez&lt;/a&gt; for introducing me to Dask, and thanks to &lt;a class="reference external" href="https://sakura9096.github.io/"&gt;Claire Zhang&lt;/a&gt; for finding the survey of collaborative filtering systems.&lt;/p&gt;
&lt;/div&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="recommender systems"></category><category term="dask"></category><category term="pandas"></category><category term="big data"></category></entry><entry><title>Taking Advantage of Sparsity in the ALS-WR Algorithm</title><link href="http://benjlindsay.com/blog/taking-advantage-of-sparsity-in-als-wr-algorithm/" rel="alternate"></link><published>2017-02-11T23:25:00-05:00</published><updated>2017-02-11T23:25:00-05:00</updated><author><name>Ben Lindsay</name></author><id>tag:benjlindsay.com,2017-02-11:/blog/taking-advantage-of-sparsity-in-als-wr-algorithm/</id><summary type="html">&lt;p class="first last"&gt;The ALS-WR algorithm works well for recommender systems involving a sparse matrix of users by items to review, which happens when most people only review a small subset of many possible items (businesses, movies, etc.). By tweaking the code from a great tutorial to take advantage of this sparsity, I was able to dramatically reduce the computation time.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;As a part of the &lt;a class="reference external" href="http://penndsg.com"&gt;Penn Data Science Group&lt;/a&gt; which I co-founded with &lt;a class="reference external" href="http://jennhwang.me/"&gt;Jenn Hwang&lt;/a&gt;, I'm starting on a team project aimed at developing a recommender system using Yelp data provided for the &lt;a class="reference external" href="https://www.yelp.com/dataset_challenge"&gt;Yelp Dataset Challenge&lt;/a&gt;. Since the Alternating-Least-Squares with Weighted-&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;-Regularization (ALS-WR) algorithm seems to be a popular algorithm for recommender systems, I'm testing it on our Yelp dataset. It was developed for the &lt;a class="reference external" href="http://www.netflixprize.com/"&gt;Netflix Prize&lt;/a&gt; competition, which also involved a sparse matrix of reviewers by items being reviewed.&lt;/p&gt;
&lt;p&gt;While searching for resources on the ALS-WR algorithm, I came across &lt;a class="reference external" href="http://online.cambridgecoding.com/notebooks/mhaller/predicting-user-preferences-in-python-using-alternating-least-squares"&gt;an excellent tutorial&lt;/a&gt; that walks you through the theory and how to implement the algorithm using python on a small dataset of movie reviews. It even provides a &lt;a class="reference external" href="https://s3-eu-west-1.amazonaws.com/com.cambridgecoding.students/mhaller/notebooks/654ddb1334a7f8246ca48d91dd98b653/notebook.ipynb"&gt;link to download a Jupyter Notebook&lt;/a&gt; that you can run and see the algorithm in action. Having this notebook to toy around with was extremely helpful in familiarizing myself with the algorithm. However, as I compared the code in the notebook to the math in the blog post and in the &lt;a class="reference external" href="http://www.grappa.univ-lille3.fr/~mary/cours/stats/centrale/reco/paper/MatrixFactorizationALS.pdf"&gt;original paper&lt;/a&gt;, it seemed like it wasn't taking full advantage of the sparsity of the ratings matrix &lt;span class="math"&gt;\(R\)&lt;/span&gt;, which is a key feature of this type of problem. By slightly changing a couple lines in this code, I was able to dramatically reduce the computation time by taking advantage of the sparsity.&lt;/p&gt;
&lt;div class="section" id="the-model"&gt;
&lt;h2&gt;The Model&lt;/h2&gt;
&lt;p&gt;I won't walk through all the details because the tutorial already does that really well, but I'll give enough background to explain the change I made and why it speeds up the computation.&lt;/p&gt;
&lt;p&gt;We start with a matrix &lt;span class="math"&gt;\(R\)&lt;/span&gt; of size &lt;span class="math"&gt;\((m \times n)\)&lt;/span&gt; where each row represents one of the &lt;span class="math"&gt;\(m\)&lt;/span&gt; users and each column represents one of the &lt;span class="math"&gt;\(n\)&lt;/span&gt; movies. Most of the matrix contains 0's since most users only review a small subset of the available movies. The dataset used in the tutorial contains only about 6% nonzero values. We want to generate a low-rank approximation for &lt;span class="math"&gt;\(R\)&lt;/span&gt; such that &lt;span class="math"&gt;\(R \approx P^TQ\)&lt;/span&gt;, where &lt;span class="math"&gt;\(P^T\)&lt;/span&gt; is size &lt;span class="math"&gt;\((m \times k)\)&lt;/span&gt; and &lt;span class="math"&gt;\(Q\)&lt;/span&gt; is size &lt;span class="math"&gt;\((k \times n)\)&lt;/span&gt;, as shown below (image borrowed from the &lt;a class="reference external" href="http://online.cambridgecoding.com/notebooks/mhaller/predicting-user-preferences-in-python-using-alternating-least-squares"&gt;tutorial&lt;/a&gt;):&lt;/p&gt;
&lt;img alt="ALS-WR Matrix Schematic" src="/images/als-wr-matrix-schematic.png" /&gt;
&lt;p&gt;The columns of the resulting matrices &lt;span class="math"&gt;\(P\)&lt;/span&gt; and &lt;span class="math"&gt;\(Q\)&lt;/span&gt; turn out to contain columns with &lt;span class="math"&gt;\(k\)&lt;/span&gt; latent features about the users and movies, respectively. The &lt;span class="math"&gt;\(P\)&lt;/span&gt; and &lt;span class="math"&gt;\(Q\)&lt;/span&gt; matrices are calculated iteratively, by fixing one and solving for the other, then repeating while alternating which one is fixed. As a side note, in case you want to look at the paper, the notation is a little different. They use &lt;span class="math"&gt;\(U\)&lt;/span&gt; and &lt;span class="math"&gt;\(M\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(P\)&lt;/span&gt; and &lt;span class="math"&gt;\(Q\)&lt;/span&gt;, and &lt;span class="math"&gt;\(n_u\)&lt;/span&gt; and &lt;span class="math"&gt;\(n_m\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(m\)&lt;/span&gt; and &lt;span class="math"&gt;\(n\)&lt;/span&gt;. I'll stick with the tutorial notation in this post.&lt;/p&gt;
&lt;p&gt;The equations for solving for &lt;span class="math"&gt;\(P\)&lt;/span&gt; and &lt;span class="math"&gt;\(Q\)&lt;/span&gt; are quite similar, so let's just look at the equation for &lt;span class="math"&gt;\(P\)&lt;/span&gt;. In each iteration, the column for each user in &lt;span class="math"&gt;\(P\)&lt;/span&gt; is generated with the following equation:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\mathbf{p}_i = A_i^{-1} V_i\)&lt;/span&gt; where &lt;span class="math"&gt;\(A_i = Q_{I_i} Q_{I_i}^T + \lambda n_{p_i} E\)&lt;/span&gt; and &lt;span class="math"&gt;\(V_i = Q_{I_i} R^T(i, I_i)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;span class="math"&gt;\(E\)&lt;/span&gt; is the &lt;span class="math"&gt;\((k \times k)\)&lt;/span&gt; identity matrix, &lt;span class="math"&gt;\(n_{p_i}\)&lt;/span&gt; is the number of movies rated by user &lt;span class="math"&gt;\(i\)&lt;/span&gt;, and &lt;span class="math"&gt;\(I_i\)&lt;/span&gt; is the set of all movies rated by user &lt;span class="math"&gt;\(i\)&lt;/span&gt;. That &lt;span class="math"&gt;\(I_i\)&lt;/span&gt; in &lt;span class="math"&gt;\(Q_{I_i}\)&lt;/span&gt; and &lt;span class="math"&gt;\(R(i, I_i)\)&lt;/span&gt; means we are selecting only the columns for movies rated by user &lt;span class="math"&gt;\(i\)&lt;/span&gt;, and the way that selection is made makes all the difference.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="selecting-columns"&gt;
&lt;h2&gt;Selecting Columns&lt;/h2&gt;
&lt;p&gt;In the tutorial, the key lines to generate each &lt;span class="math"&gt;\(\mathbf{p}_i\)&lt;/span&gt; look like this:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="n"&gt;Ai&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Q&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Ii&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;Q&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;lmbda&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;nui&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;
&lt;span class="n"&gt;Vi&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Q&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Ii&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;solve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Ai&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;Vi&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Notice that in the equation for &lt;span class="math"&gt;\(A_i\)&lt;/span&gt;, the way it removes columns for movies that weren't reviewed by user &lt;span class="math"&gt;\(i\)&lt;/span&gt; is creating a &lt;span class="math"&gt;\((n \times n)\)&lt;/span&gt; matrix with the elements of &lt;span class="math"&gt;\(I_i\)&lt;/span&gt; along the diagonal, then doing a &lt;span class="math"&gt;\((n \times n) \times (n \times k)\)&lt;/span&gt; matrix multiplication between that and &lt;span class="math"&gt;\(Q^T\)&lt;/span&gt;, which zeroes out columns of &lt;span class="math"&gt;\(Q\)&lt;/span&gt; for movies user &lt;span class="math"&gt;\(i\)&lt;/span&gt; did not review. This matrix multiplication is an expensive operation that (naively) has a complexity of &lt;span class="math"&gt;\(O(kn^2)\)&lt;/span&gt; (although probably a bit better with the &lt;em&gt;numpy&lt;/em&gt; implementation). A similar operation is done in the &lt;span class="math"&gt;\(V_i\)&lt;/span&gt; calculation. Even though this is not as expensive (complexity of &lt;span class="math"&gt;\(O(n^2)\)&lt;/span&gt;), that's still an operation we'd like to avoid if possible.&lt;/p&gt;
&lt;p&gt;On reading the equations and Matlab algorithm implementation in the original paper, I noticed that rather than zeroing out unwanted columns, they actually remove those columns by creating a submatrix of &lt;span class="math"&gt;\(Q\)&lt;/span&gt; and a subvector of &lt;span class="math"&gt;\(\mathbf{r}_i\)&lt;/span&gt;. This does 2 important things: First, it lets us remove that inner matrix multiplications. Second, it dramatically reduces the cost of the remaining matrix multiplications. Since we have a density of only about 6% in our &lt;span class="math"&gt;\(R\)&lt;/span&gt; matrix, the cost of both &lt;span class="math"&gt;\(Q_{I_i}Q_{I_i}^T\)&lt;/span&gt; and &lt;span class="math"&gt;\(Q_{I_i}R^T(i,I_i)\)&lt;/span&gt; should theoretically be reduced to about 6% of their original costs, since the complexities of those operations (&lt;span class="math"&gt;\(O(nk^2)\)&lt;/span&gt; and &lt;span class="math"&gt;\(O(nk)\)&lt;/span&gt;) are both linearly dependent on &lt;span class="math"&gt;\(n\)&lt;/span&gt;. Here's the code that replaces the 3 lines shown above:&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="c1"&gt;# Get array of nonzero indices in row Ii&lt;/span&gt;
&lt;span class="n"&gt;Ii_nonzero&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nonzero&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Ii&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Select subset of Q associated with movies reviewed by user i&lt;/span&gt;
&lt;span class="n"&gt;Q_Ii&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Q&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;Ii_nonzero&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="c1"&gt;# Select subset of row R_i associated with movies reviewed by user i&lt;/span&gt;
&lt;span class="n"&gt;R_Ii&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;R&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Ii_nonzero&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;Ai&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Q_Ii&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Q_Ii&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;lmbda&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;nui&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;E&lt;/span&gt;
&lt;span class="n"&gt;Vi&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Q_Ii&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;R_Ii&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;solve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Ai&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Vi&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;By making that replacement and a similar one for the equations for &lt;span class="math"&gt;\(\mathbf{q}_j\)&lt;/span&gt;, a series of 15 iterations went from taking 15-16 minutes down to about 13 seconds—a ~70-fold speedup! Check out &lt;a class="reference external" href="https://github.com/benlindsay/als-wr-tutorial/blob/master/modified_notebook.ipynb"&gt;the notebook with my updates&lt;/a&gt; on GitHub, or clone the whole &lt;a class="reference external" href="https://github.com/benlindsay/als-wr-tutorial"&gt;repo&lt;/a&gt; to run it yourself.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusions"&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;The moral of the story here is that sometimes things that don't seem like a big deal at first glance can make huge changes in the performance of your algorithms. This exercise reinforced in my mind the value of spending a little extra time to make sure you understand the algorithm or tool you're using. And more specifically, if you have a sparse dataset, make that sparsity work for you.&lt;/p&gt;
&lt;/div&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="als-wr"></category><category term="python"></category><category term="machine learning"></category><category term="recommender systems"></category></entry><entry><title>Dealing with Grid Data in Python</title><link href="http://benjlindsay.com/blog/dealing-with-grid-data-in-python/" rel="alternate"></link><published>2016-12-08T16:04:00-05:00</published><updated>2016-12-08T16:04:00-05:00</updated><author><name>Ben Lindsay</name></author><id>tag:benjlindsay.com,2016-12-08:/blog/dealing-with-grid-data-in-python/</id><summary type="html">&lt;p class="first last"&gt;In my PhD research, I do a lot of analysis of 2D and 3D grid data output by simulations I run. In my analyses, it's very helpful to restructure these data into a more useable format. A few key lines of python code do the trick.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In my PhD research, I do a lot of analysis of 2D and 3D grid data output by simulations I run. If, for example, I had a toy system that was 3x2x2 grid points, the raw data would be structured sort of like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;x   y   z   value
0   0   0   0.9
1   0   0   1.1
2   0   0   0.8
0   1   0   1.1
1   1   0   1.0
2   1   0   0.9
0   0   1   0.6
1   0   1   1.2
2   0   1   0.8
0   1   1   0.9
1   1   1   1.2
2   1   1   1.3
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In my analyses, it's very helpful to restructure these data into a format where, in this case, &lt;code class="python"&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;/code&gt;, &lt;code class="python"&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;/code&gt;, &lt;code class="python"&gt;&lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;/code&gt;, and &lt;code class="python"&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;/code&gt; is a 3D array such that &lt;code class="python"&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;/code&gt; returns the value corresponding to position &lt;code class="python"&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It's easy to do that in just a few lines. Say the above raw data is stored in &lt;code class="python"&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dat&lt;/span&gt;&lt;/code&gt;.&lt;/p&gt;
&lt;pre class="code python literal-block"&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loadtxt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'data.dat'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;skiprows&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;nx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ny&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nz&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;nz&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ny&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nx&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Note that if the raw data had &lt;code class="python"&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;/code&gt; varying the slowest and &lt;code class="python"&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;/code&gt; varying the fastest, the final line would look like &lt;code class="python"&gt;&lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;nx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ny&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nz&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, if you want to go the other way, where you have your &lt;code class="python"&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;/code&gt;, &lt;code class="python"&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;/code&gt;, and &lt;code class="python"&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;/code&gt; arrays and 3D &lt;code class="python"&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;/code&gt; array, you can make use of the &lt;code class="python"&gt;&lt;span class="n"&gt;sklearn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extmath&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cartesian&lt;/span&gt;&lt;/code&gt; function (first introduced on this &lt;a class="reference external" href="http://stackoverflow.com/a/1235363/2680824"&gt;StackOverflow post&lt;/a&gt;. If you want &lt;code class="python"&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;/code&gt; to be the fastest changing variable, it would look something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.utils.extmath&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;cartesian&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;nx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ny&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nz&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nx&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;ny&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;nz&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;nx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;ny&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;nz&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# define 3D value array&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;xyz&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cartesian&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hstack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;xyz&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code class="python"&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;/code&gt; thing on the last line adds an extra dimension to the 1D value array so the elements of the tuple passed to &lt;code class="python"&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hstack&lt;/span&gt;&lt;/code&gt; are both 2D numpy arrays.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,:]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
</content><category term="python"></category><category term="numpy"></category></entry><entry><title>Parameter Sweep Bash Script</title><link href="http://benjlindsay.com/blog/parameter-sweep-bash-script/" rel="alternate"></link><published>2015-12-19T21:25:00-05:00</published><updated>2015-12-19T21:25:00-05:00</updated><author><name>Ben Lindsay</name></author><id>tag:benjlindsay.com,2015-12-19:/blog/parameter-sweep-bash-script/</id><summary type="html">&lt;p class="first last"&gt;In my polymer field theory research, often my studies involve running a bunch of simulations where I pick one or more input parameters and change them over a range of values, then compare the results of each separate simulation to see how that/those variable(s) affect the system I’m simulating. This kind of study is called a “parameter sweep”, and can also be referred to as “embarrassingly parallel”, because the processor(s) for each for each individual job don’t need to communicate with the processor(s) from any other job. It can be very tedious to manually create input files for each job, so I wrote a bash script to help me out.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;In my polymer field theory research, often my studies involve running a bunch of simulations where I pick one or more input parameters and change them over a range of values, then compare the results of each separate simulation to see how that/those variable(s) affect the system I'm simulating. This kind of study is called a &amp;quot;parameter sweep&amp;quot;, and can also be referred to as &amp;quot;embarrassingly parallel&amp;quot;, because the processor(s) for each for each individual job don't need to communicate with the processor(s) from any other job. It can be very tedious to manually create input files for each job, so I wrote a bash script to help me out.&lt;/p&gt;
&lt;p&gt;For example, if I want to simulate 3 different polymer nanocomposite systems, each with a different nanorod length, I could manually create 3 directories like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mkdir length1
mkdir length2
mkdir length3
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then I could copy an input file, &lt;code&gt;bcp.input&lt;/code&gt;, and a submit file, &lt;code&gt;sub.sh&lt;/code&gt; into each of those folders like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; d in length*/ &lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
  cp bcp.input &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$d&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
  cp sub.sh &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$d&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then I could proceed to manually edit all 6 files (or just 3 if the submission script doesn't have to change). If it's just 3 files, it's not too bad, but if I want to run 10 or 20 simulations with slight changes in the input file for each one, manual editing gets real tedious real fast. I got fed up with it and wrote a script to do all the editing for me. The script is called &lt;code&gt;param-sweep.sh&lt;/code&gt;. Feel free to look at it on &lt;a class="reference external" href="https://bitbucket.org/benlindsay/param-sweep.git"&gt;Bitbucket&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Before running the script, I make a template for the input file and submission script with parameter names that the script will replace with parameter values. My input file template could look something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="m"&gt;1000&lt;/span&gt;        &lt;span class="c1"&gt;# Number of iterations&lt;/span&gt;
&lt;span class="m"&gt;60&lt;/span&gt;          &lt;span class="c1"&gt;# Polymer length&lt;/span&gt;
&lt;span class="m"&gt;1&lt;/span&gt;           &lt;span class="c1"&gt;# Nanorod radius&lt;/span&gt;
NRLENGTH    &lt;span class="c1"&gt;# Nanorod length&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and my submission script template could look something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/sh&lt;/span&gt;
&lt;span class="c1"&gt;#PBS -N TRIALNAME&lt;/span&gt;
&lt;span class="c1"&gt;#PBS -l nodes=1:ppn=12&lt;/span&gt;
&lt;span class="c1"&gt;#PBS -l walltime=01:00:00,mem=2gb&lt;/span&gt;

&lt;span class="nb"&gt;cd&lt;/span&gt; &lt;span class="nv"&gt;$PBS_O_WORKDIR&lt;/span&gt;

&lt;span class="c1"&gt;# Run code that looks for bcp.input in the current directory&lt;/span&gt;
mpirun &lt;span class="nv"&gt;$HOME&lt;/span&gt;/code/awesome_code.exe
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In this example, I want to replace &lt;code&gt;NRLENGTH&lt;/code&gt; with the actual nanorod length for each &lt;code&gt;bcp.input&lt;/code&gt; file in &lt;code&gt;./length1&lt;/code&gt;, &lt;code&gt;./length2&lt;/code&gt;, and &lt;code&gt;./length3&lt;/code&gt;, and I want to replace &lt;code&gt;TRIALNAME&lt;/code&gt; with a name corresponding to each simulation in each &lt;code&gt;sub.sh&lt;/code&gt; file. The script does this by looking through a &lt;code&gt;trials.txt&lt;/code&gt; file I make that would look like this in this case:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;name        i:NRLENGTH  s:TRIALNAME
length1     4           length1-trial
length2     5           length2-trial
length3     6           length3-trial
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;i:&lt;/code&gt; and &lt;code&gt;s:&lt;/code&gt; before &lt;code&gt;NRLENGTH&lt;/code&gt; and &lt;code&gt;TRIALNAME&lt;/code&gt;, respectively, tell the script to look in the input file or submission script for each variable. Finally, let's look at how to use the script:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="hll"&gt;$ ls
&lt;/span&gt;bcp.input   trials.txt  sub.sh
&lt;span class="hll"&gt;$ ~/scripts/param-sweep.sh -t trials.txt -i bcp.input -s sub.sh
&lt;/span&gt;Trials file:        trials.txt
Input file:         bcp.input
Submission script:  sub.sh
&lt;span class="m"&gt;3&lt;/span&gt; trials
&lt;span class="m"&gt;2&lt;/span&gt; vars
Submitting trial length1:
&lt;span class="m"&gt;1443364&lt;/span&gt;.rrlogin.internal
Submitting trial length2:
&lt;span class="m"&gt;1443365&lt;/span&gt;.rrlogin.internal
Submitting trial length3:
&lt;span class="m"&gt;1443366&lt;/span&gt;.rrlogin.internal
&lt;span class="hll"&gt;$ tree
&lt;/span&gt;.
├── bcp.input
├── length1
│   ├── bcp.input
│   └── sub.sh
├── length2
│   ├── bcp.input
│   └── sub.sh
├── length3
│   ├── bcp.input
│   └── sub.sh
├── sub.sh
└── trials.txt
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So the script made directories for all three simulations, replaced &lt;code&gt;NRLENGTH&lt;/code&gt; with &lt;code&gt;4&lt;/code&gt;, &lt;code&gt;5&lt;/code&gt;, and &lt;code&gt;6&lt;/code&gt; in the &lt;code&gt;bcp.input&lt;/code&gt; files, replaced &lt;code&gt;TRIALNAME&lt;/code&gt; with &lt;code&gt;length1-trial&lt;/code&gt;, &lt;code&gt;length2-trial&lt;/code&gt;, and &lt;code&gt;length3-trial&lt;/code&gt; in the &lt;code&gt;sub.sh&lt;/code&gt; files, and submitted the &lt;code&gt;sub.sh&lt;/code&gt; files from within their respective simulation directories. In this case, since my script expects files with the names I used, I could have just typed &lt;code&gt;~/scripts/param-sweep.sh&lt;/code&gt;. If I wanted to be able to check the files before submitting, I could have typed &lt;code&gt;~/scripts/param-sweep.sh -n&lt;/code&gt; which would create the directories and files without submitting the jobs.&lt;/p&gt;
&lt;p&gt;A few caveats: the script isn't currently set up to handle more than one layer of simulation directories. Also, the script as it's set up right now copies whatever input file and submission script its fed to files named &lt;code&gt;bcp.input&lt;/code&gt; and &lt;code&gt;sub.sh&lt;/code&gt;. Finally, you'll need to make sure that the variable name you want the script to find and replace with variable values doesn't show up anywhere else in the file. The script will find and replace all instances of the variable name (case sensitive).&lt;/p&gt;
&lt;p&gt;This script has saved me a lot of time. Hopefully it can help someone else out there too.&lt;/p&gt;
</content><category term="bash"></category><category term="productivity"></category></entry></feed>